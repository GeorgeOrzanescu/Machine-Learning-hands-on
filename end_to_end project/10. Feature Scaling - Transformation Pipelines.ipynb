{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling and Transformation Pipelines\n",
    "### One of the most important transformations you need to apply to your data is feature scaling.\n",
    "<br> **Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales**<br>_This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Note that scaling the target values is generally not required._\n",
    "\n",
    "<br> **There are two common ways to get all attributes to have the same scale**: \n",
    "        <br>- min-maxscaling : also known as NORMALIZATION\n",
    "        \n",
    "            - values are shifted and rescaled so that they end up ranging from 0 to 1\n",
    "            - We do this by subtracting the min value and dividing by the max minus the min.\n",
    "            - Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if you don’t want 0–1 for some reason.\n",
    "            \n",
    "   <br>- Standardization :\n",
    "   \n",
    "           - first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation so that the resulting distribution has unit variance.\n",
    "           - Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1).\n",
    "           - standardization is much less affected by outliers;\n",
    "           - For example, suppose a district had a median income equal to 100 (by mistake). Min-max scaling would then crush all the other values from 0–15 down to 0–0.15, whereas standardization would not be much affected;\n",
    "           - Scikit-Learn provides a transformer called StandardScaler for standardization.\n",
    "        \n",
    "### !!!! As with all the transformations, it is important to fit the scalers to the training data only, not to the full dataset (including the test set). Only then can you use them to transform the training set and the test set (and new data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "housing = pd.read_csv(r\"C:\\Users\\georg\\Desktop\\Machine Learning\\notebooks_detailed\\datasets\\housing\\housing.csv\") \n",
    "\n",
    "housing[\"income_category\"] =pd.cut(housing[\"median_income\"],bins=[0,1.5,3.0,4.5,6.,np.inf],labels=[1,2,3,4,5])\n",
    "split_indices = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n",
    "for train_index, test_index in split_indices.split(housing,housing[\"income_category\"]): \n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index] \n",
    "# standard by now\n",
    "\n",
    "for set_ in (strat_train_set, strat_test_set):  ## we are dropping the new attribute\n",
    "    set_.drop(\"income_category\", axis=1, inplace=True)  \n",
    "    \n",
    "\n",
    "housing = strat_train_set.copy()  # make a copy of original data \n",
    "\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy() # seprate the target column\n",
    "\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1) # numerical attributes\n",
    "housing_cat = housing[[\"ocean_proximity\"]] # categorical attributes\n",
    "\n",
    "\n",
    "# this is a very condesated form , don\"t worry about it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As you can see, there are many data transformation steps that need to be executed in the right order.\n",
    "## Scikit-Learn provides the Pipeline class to help withsuch sequences of transformations.\n",
    "\n",
    "**Pipeline** is just an utility that helps you sequence different transformations ( find set of features,generate new features ,select only some good features etc) of the original dataset before applying a final estimator.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
