{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling and Transformation Pipelines\n",
    "### One of the most important transformations you need to apply to your data is feature scaling.\n",
    "<br> **Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales**<br>_This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Note that scaling the target values is generally not required._\n",
    "\n",
    "<br> **There are two common ways to get all attributes to have the same scale**: \n",
    "        <br>- min-maxscaling : also known as NORMALIZATION\n",
    "        \n",
    "            - values are shifted and rescaled so that they end up ranging from 0 to 1\n",
    "            - We do this by subtracting the min value and dividing by the max minus the min.\n",
    "            - Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if you don’t want 0–1 for some reason.\n",
    "            \n",
    "   <br>- Standardization :\n",
    "   \n",
    "           - first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation so that the resulting distribution has unit variance.\n",
    "           - Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1).\n",
    "           - standardization is much less affected by outliers;\n",
    "           - For example, suppose a district had a median income equal to 100 (by mistake). Min-max scaling would then crush all the other values from 0–15 down to 0–0.15, whereas standardization would not be much affected;\n",
    "           - Scikit-Learn provides a transformer called StandardScaler for standardization.\n",
    "        \n",
    "### !!!! As with all the transformations, it is important to fit the scalers to the training data only, not to the full dataset (including the test set). Only then can you use them to transform the training set and the test set (and new data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "housing = pd.read_csv(r\"C:\\Users\\georg\\Desktop\\Machine Learning\\notebooks_detailed\\datasets\\housing\\housing.csv\") \n",
    "\n",
    "housing[\"income_category\"] =pd.cut(housing[\"median_income\"],bins=[0,1.5,3.0,4.5,6.,np.inf],labels=[1,2,3,4,5])\n",
    "split_indices = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n",
    "for train_index, test_index in split_indices.split(housing,housing[\"income_category\"]): \n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index] \n",
    "# standard by now\n",
    "\n",
    "for set_ in (strat_train_set, strat_test_set):  ## we are dropping the new attribute\n",
    "    set_.drop(\"income_category\", axis=1, inplace=True)  \n",
    "    \n",
    "\n",
    "housing = strat_train_set.copy()  # make a copy of original data \n",
    "\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy() # seprate the target column\n",
    "\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1) # numerical attributes\n",
    "housing_cat = housing[[\"ocean_proximity\"]] # categorical attributes\n",
    "\n",
    "\n",
    "# this is a very condesated form , don't worry about it"
   ]
  },
  {
   "source": [
    "### So lets see how the Normalization works\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.03973139],\n",
       "       [0.01711858],\n",
       "       [0.04949891],\n",
       "       ...,\n",
       "       [0.12334029],\n",
       "       [0.0497024 ],\n",
       "       [0.07857252]])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "normalization = MinMaxScaler() #instantiate\n",
    "normalized_feature = normalization.fit_transform(housing_num[[\"total_rooms\"]])  #fit_transform method on target\n",
    "normalized_feature"
   ]
  },
  {
   "source": [
    "### Normalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Standardization now\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.63621141],\n",
       "       [-0.99833135],\n",
       "       [-0.43363936],\n",
       "       ...,\n",
       "       [ 0.60790363],\n",
       "       [-0.05717804],\n",
       "       [-0.13515931]])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "standardization = StandardScaler()\n",
    "standard_feature = standardization.fit_transform(housing_num[[\"population\"]])\n",
    "standard_feature"
   ]
  },
  {
   "source": [
    "### Standardization, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Also, unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As you can see, there are many data transformation steps that need to be executed in the right order.\n",
    "## Scikit-Learn provides the Pipeline class to help withsuch sequences of transformations.\n",
    "\n",
    "**Pipeline** is just an utility that helps you sequence different transformations ( find set of features,generate new features ,select only some good features etc) of the original dataset before applying a final estimator.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "source": [
    "#### !!! We will need some imports and code from the previous notebooks !!!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-1.15604281,  0.77194962,  0.74333089, ..., -0.31205452,\n",
       "        -0.08649871,  0.15531753],\n",
       "       [-1.17602483,  0.6596948 , -1.1653172 , ...,  0.21768338,\n",
       "        -0.03353391, -0.83628902],\n",
       "       [ 1.18684903, -1.34218285,  0.18664186, ..., -0.46531516,\n",
       "        -0.09240499,  0.4222004 ],\n",
       "       ...,\n",
       "       [ 1.58648943, -0.72478134, -1.56295222, ...,  0.3469342 ,\n",
       "        -0.03055414, -0.52177644],\n",
       "       [ 0.78221312, -0.85106801,  0.18664186, ...,  0.02499488,\n",
       "         0.06150916, -0.30340741],\n",
       "       [-1.43579109,  0.99645926,  1.85670895, ..., -0.22852947,\n",
       "        -0.09586294,  0.10180567]])"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "numerical_pipeline = Pipeline([    # The Pipeline constructor takes a list of name/estimator pairs defining a sequence of steps.\n",
    "('imputer', SimpleImputer(strategy=\"median\")),\n",
    "('attribs_adder', CombinedAttributesAdder()), \n",
    "('std_scaler', StandardScaler()),\n",
    "])\n",
    "#The names can be anything you like (as long as they are unique and don’t contain double underscores “__”): they will come in handy later for hyperparameter tuning.\n",
    "\n",
    "housing_numerical_transformed = numerical_pipeline.fit_transform(housing_num)\n",
    "housing_numerical_transformed\n"
   ]
  },
  {
   "source": [
    "### We can do this for the categorical data to but it would be more convenient to have a single transformer able to handle all columns, applying the appropriate transformations to each column.\n",
    "### Scikit-Learn introduced the ColumnTransformer\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-1.15604281,  0.77194962,  0.74333089, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.17602483,  0.6596948 , -1.1653172 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.18684903, -1.34218285,  0.18664186, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       ...,\n",
       "       [ 1.58648943, -0.72478134, -1.56295222, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.78221312, -0.85106801,  0.18664186, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.43579109,  0.99645926,  1.85670895, ...,  0.        ,\n",
       "         1.        ,  0.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "num_attribs = list(housing_num)  # list of numerical columns \n",
    "cat_attribs = [\"ocean_proximity\"] #list of categorical columns \n",
    "\n",
    "full_pipeline = ColumnTransformer([ # The constructor requires a list of tuples, where each tuple contains a name, a transformer and a list of names (or indices) of columns that the transformer should be applied to\n",
    "(\"num\", numerical_pipeline, num_attribs), # name : whatever u want| transfomer : numerical_pipeline defined earlier|target:num_attribs\n",
    "(\"cat\", OneHotEncoder(), cat_attribs),\n",
    "])\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "housing_prepared"
   ]
  },
  {
   "source": [
    "!!! An important parameter of ColumnTransformer estimator is **remainder** which can be {‘drop’, ‘passthrough’}\n",
    "<br> drop - allows u to drop any column/s u like \n",
    "<br> passthrough - it leaves the column/s specified untouched\n",
    "<br> !!! Also is worth mentioning that ColumnTransformer automatically concatonates all output from there specific transformer !!!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}