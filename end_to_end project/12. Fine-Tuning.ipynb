{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-1.15604281,  0.77194962,  0.74333089, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.17602483,  0.6596948 , -1.1653172 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.18684903, -1.34218285,  0.18664186, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       ...,\n",
       "       [ 1.58648943, -0.72478134, -1.56295222, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.78221312, -0.85106801,  0.18664186, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.43579109,  0.99645926,  1.85670895, ...,  0.        ,\n",
       "         1.        ,  0.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# _________________________________________________________________________________________________________\n",
    "\n",
    "housing = pd.read_csv(r\"C:\\Users\\georg\\Desktop\\Machine Learning\\notebooks_detailed\\datasets\\housing\\housing.csv\") \n",
    "\n",
    "housing[\"income_category\"] =pd.cut(housing[\"median_income\"],bins=[0,1.5,3.0,4.5,6.,np.inf],labels=[1,2,3,4,5])\n",
    "split_indices = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n",
    "for train_index, test_index in split_indices.split(housing,housing[\"income_category\"]): \n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "    \n",
    "for set_ in (strat_train_set, strat_test_set):  ## we are dropping the new attribute\n",
    "    set_.drop(\"income_category\", axis=1, inplace=True)  \n",
    "    \n",
    "\n",
    "housing = strat_train_set.copy()  # make a copy of original data \n",
    "\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy() # seprate the target column\n",
    "\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1) # numerical attributes\n",
    "housing_cat = housing[[\"ocean_proximity\"]] # categorical attributes\n",
    "\n",
    "\n",
    "# this is a very condesated form , don't worry about it\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "            \n",
    "numerical_pipeline = Pipeline([    # The Pipeline constructor takes a list of name/estimator pairs defining a sequence of steps.\n",
    "('imputer', SimpleImputer(strategy=\"median\")),\n",
    "('attribs_adder', CombinedAttributesAdder()), \n",
    "('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "num_attribs = list(housing_num)  # list of numerical columns \n",
    "cat_attribs = [\"ocean_proximity\"] #list of categorical columns \n",
    "\n",
    "full_pipeline = ColumnTransformer([ # The constructor requires a list of tuples, where each tuple contains a name, a transformer and a list of names (or indices) of columns that the transformer should be applied to\n",
    "(\"num\", numerical_pipeline, num_attribs), # name : whatever u want| transfomer : numerical_pipeline defined earlier|target:num_attribs\n",
    "(\"cat\", OneHotEncoder(), cat_attribs),\n",
    "])\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "housing_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "18603.515021376355\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_regression = RandomForestRegressor(n_estimators=100,random_state=42)\n",
    "forest_regression.fit(housing_prepared,housing_labels)\n",
    "housing_predictions_forest = forest_regression.predict(housing_prepared)\n",
    "print(mean_squared_error(housing_labels,housing_predictions_forest,squared=False)) #score on the training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([49519.80364233, 47461.9115823 , 50029.02762854, 52325.28068953,\n",
       "       49308.39426421, 53446.37892622, 48634.8036574 , 47585.73832311,\n",
       "       53490.10699751, 50021.5852922 ])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "forest_scores = cross_val_score(forest_regression,housing_prepared,housing_labels,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "forest_scores_RMSE = np.sqrt(-forest_scores)\n",
    "forest_scores_RMSE  #scores on the validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is out best model. So now we need to fine tune it. \n",
    "### One way to do that would be to fiddle with the hyperparameters manually, until you find a great combination of hyperparameter values.\n",
    "### Instead you should get Scikit-Learn’s GridSearchCV to search for you. All you need to is tell it which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameter values, using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**param_grid parameter is the most important** :  it accepts a list or a dictionary of hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{\"n_estimators\" :[3,10,30],\"max_features\":[2,4,6,8]}, # try 12 (3×4) combinations of hyperparameters \n",
    "              {\"bootstrap\":[False],\"n_estimators\":[3,10],\"max_features\":[2,3,4]}] \n",
    "             # then try 6 (2×3) combinations with bootstrap set as False\n",
    "forest_reg =RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each decision tree in the ensemble is fit on a bootstrap sample drawn from the training dataset. This can be turned off by setting the “bootstrap” argument to False, if you desire. In that case, the whole training dataset will be used to train each decision tree. \n",
    "\n",
    "All in all, the grid search will explore 12 + 6 = 18 combinations of RandomForestRegressor hyperparameter values, and it will train each model five times (since we are using five-fold cross validation). In other words, all in all, there will be 18 × 5 = 90 rounds of training! It may take quite a long time, but when it is done you can get the best combination of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestRegressor(),\n",
       "             param_grid=[{'max_features': [2, 4, 6, 8],\n",
       "                          'n_estimators': [3, 10, 30]},\n",
       "                         {'bootstrap': [False], 'max_features': [2, 3, 4],\n",
       "                          'n_estimators': [3, 10]}],\n",
       "             return_train_score=True, scoring='neg_mean_squared_error')"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'max_features': 6, 'n_estimators': 30}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "grid_search.best_params_    # best hyperparameter combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features=6, n_estimators=30)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since the best combination is or max values , we can try again to search with higher values and maybe the score will improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "64835.363644927624 {'max_features': 2, 'n_estimators': 3}\n55403.54864273394 {'max_features': 2, 'n_estimators': 10}\n52716.910287490864 {'max_features': 2, 'n_estimators': 30}\n59113.86194194443 {'max_features': 4, 'n_estimators': 3}\n53063.119868028065 {'max_features': 4, 'n_estimators': 10}\n50735.30468120093 {'max_features': 4, 'n_estimators': 30}\n59933.86840051729 {'max_features': 6, 'n_estimators': 3}\n52121.86149777997 {'max_features': 6, 'n_estimators': 10}\n50000.74005700329 {'max_features': 6, 'n_estimators': 30}\n58341.14731268872 {'max_features': 8, 'n_estimators': 3}\n52032.09710860755 {'max_features': 8, 'n_estimators': 10}\n50295.733224025455 {'max_features': 8, 'n_estimators': 30}\n62010.72908073058 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n54805.54177386228 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n59757.20247402998 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n53008.4978146372 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n58468.447430859436 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n51568.88488738203 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "## See all scores \n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.05121117, 0.16543708, 0.49231   , 0.08081832, 0.26745949,\n",
       "        0.82838526, 0.11062469, 0.37268343, 1.11665006, 0.14923344,\n",
       "        0.48710866, 1.42571945, 0.08541927, 0.26906047, 0.10862398,\n",
       "        0.35067897, 0.13262939, 0.43889852]),\n",
       " 'std_fit_time': array([0.00116597, 0.0025776 , 0.0035449 , 0.00074837, 0.00531543,\n",
       "        0.05472401, 0.00185519, 0.00683087, 0.01405497, 0.00407023,\n",
       "        0.01282452, 0.01118508, 0.00224524, 0.00660457, 0.00224566,\n",
       "        0.00294   , 0.00307309, 0.00594785]),\n",
       " 'mean_score_time': array([0.00260053, 0.00720162, 0.02000465, 0.00260024, 0.0072021 ,\n",
       "        0.02080483, 0.00200057, 0.00720196, 0.02020431, 0.00320082,\n",
       "        0.00720196, 0.02000461, 0.00300074, 0.00820203, 0.00280075,\n",
       "        0.00860167, 0.0028007 , 0.00840197]),\n",
       " 'std_score_time': array([4.89862464e-04, 4.00567179e-04, 2.43140197e-07, 4.90115593e-04,\n",
       "        4.00090569e-04, 1.16633486e-03, 1.50789149e-07, 3.99804382e-04,\n",
       "        4.00424185e-04, 4.00400403e-04, 4.00161885e-04, 8.86968386e-07,\n",
       "        2.61174468e-07, 4.00209498e-04, 4.00090228e-04, 4.89882246e-04,\n",
       "        4.00185681e-04, 4.89901382e-04]),\n",
       " 'param_max_features': masked_array(data=[2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8, 2, 2, 3, 3, 4, 4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[3, 10, 30, 3, 10, 30, 3, 10, 30, 3, 10, 30, 3, 10, 3,\n",
       "                    10, 3, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_bootstrap': masked_array(data=[--, --, --, --, --, --, --, --, --, --, --, --, False,\n",
       "                    False, False, False, False, False],\n",
       "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                     True,  True,  True,  True, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_features': 2, 'n_estimators': 3},\n",
       "  {'max_features': 2, 'n_estimators': 10},\n",
       "  {'max_features': 2, 'n_estimators': 30},\n",
       "  {'max_features': 4, 'n_estimators': 3},\n",
       "  {'max_features': 4, 'n_estimators': 10},\n",
       "  {'max_features': 4, 'n_estimators': 30},\n",
       "  {'max_features': 6, 'n_estimators': 3},\n",
       "  {'max_features': 6, 'n_estimators': 10},\n",
       "  {'max_features': 6, 'n_estimators': 30},\n",
       "  {'max_features': 8, 'n_estimators': 3},\n",
       "  {'max_features': 8, 'n_estimators': 10},\n",
       "  {'max_features': 8, 'n_estimators': 30},\n",
       "  {'bootstrap': False, 'max_features': 2, 'n_estimators': 3},\n",
       "  {'bootstrap': False, 'max_features': 2, 'n_estimators': 10},\n",
       "  {'bootstrap': False, 'max_features': 3, 'n_estimators': 3},\n",
       "  {'bootstrap': False, 'max_features': 3, 'n_estimators': 10},\n",
       "  {'bootstrap': False, 'max_features': 4, 'n_estimators': 3},\n",
       "  {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}],\n",
       " 'split0_test_score': array([-3.91273508e+09, -2.96965506e+09, -2.63882789e+09, -3.40475055e+09,\n",
       "        -2.73792396e+09, -2.38676185e+09, -3.51683092e+09, -2.63915148e+09,\n",
       "        -2.34366707e+09, -3.11369444e+09, -2.48588021e+09, -2.37689105e+09,\n",
       "        -3.68985133e+09, -2.71615211e+09, -3.23596159e+09, -2.64901335e+09,\n",
       "        -3.47835489e+09, -2.56207658e+09]),\n",
       " 'split1_test_score': array([-4.32819428e+09, -3.12644108e+09, -2.86502599e+09, -3.49080107e+09,\n",
       "        -2.92267739e+09, -2.69327952e+09, -3.67976493e+09, -2.73352279e+09,\n",
       "        -2.54150578e+09, -3.58013980e+09, -2.76701037e+09, -2.58774362e+09,\n",
       "        -4.01065172e+09, -3.19792517e+09, -3.73869297e+09, -2.90505800e+09,\n",
       "        -3.46053283e+09, -2.57518750e+09]),\n",
       " 'split2_test_score': array([-4.24679190e+09, -3.19871706e+09, -2.90656387e+09, -3.54673507e+09,\n",
       "        -2.94533381e+09, -2.73004944e+09, -3.77588621e+09, -2.80618527e+09,\n",
       "        -2.60575992e+09, -3.55577898e+09, -2.82692698e+09, -2.66151914e+09,\n",
       "        -3.89394541e+09, -3.22136745e+09, -3.69025215e+09, -2.88754008e+09,\n",
       "        -3.45817189e+09, -2.86487879e+09]),\n",
       " 'split3_test_score': array([-4.22416124e+09, -2.86359038e+09, -2.60977750e+09, -3.58436922e+09,\n",
       "        -2.56976157e+09, -2.39346611e+09, -3.48852147e+09, -2.53026344e+09,\n",
       "        -2.34918752e+09, -3.38835578e+09, -2.55125616e+09, -2.32366865e+09,\n",
       "        -3.61924360e+09, -2.75974866e+09, -3.51919873e+09, -2.75221372e+09,\n",
       "        -3.20666003e+09, -2.54805772e+09]),\n",
       " 'split4_test_score': array([-4.30623939e+09, -3.18936243e+09, -2.87516790e+09, -3.44558746e+09,\n",
       "        -2.90277672e+09, -2.66679878e+09, -3.49933939e+09, -2.87431926e+09,\n",
       "        -2.66024974e+09, -3.38047836e+09, -2.90562193e+09, -2.69848145e+09,\n",
       "        -4.01296055e+09, -3.12304366e+09, -3.67051079e+09, -2.85567905e+09,\n",
       "        -3.48907709e+09, -2.74654885e+09]),\n",
       " 'mean_test_score': array([-4.20362438e+09, -3.06955320e+09, -2.77907263e+09, -3.49444867e+09,\n",
       "        -2.81569469e+09, -2.57407114e+09, -3.59206858e+09, -2.71668845e+09,\n",
       "        -2.50007401e+09, -3.40368947e+09, -2.70733913e+09, -2.52966078e+09,\n",
       "        -3.84533052e+09, -3.00364741e+09, -3.57092325e+09, -2.80990084e+09,\n",
       "        -3.41855934e+09, -2.65934989e+09]),\n",
       " 'std_test_score': array([1.50299226e+08, 1.31661767e+08, 1.27440750e+08, 6.52166379e+07,\n",
       "        1.43084775e+08, 1.51552782e+08, 1.15291791e+08, 1.21578629e+08,\n",
       "        1.30974981e+08, 1.66815426e+08, 1.61606886e+08, 1.51678615e+08,\n",
       "        1.63146198e+08, 2.19791480e+08, 1.82854306e+08, 9.63233815e+07,\n",
       "        1.06565012e+08, 1.25520625e+08]),\n",
       " 'rank_test_score': array([18, 11,  7, 14,  9,  3, 16,  6,  1, 12,  5,  2, 17, 10, 15,  8, 13,\n",
       "         4]),\n",
       " 'split0_train_score': array([-1.09771362e+09, -5.68115439e+08, -4.43186201e+08, -9.27746216e+08,\n",
       "        -5.50545524e+08, -3.95587979e+08, -9.82762171e+08, -4.88938347e+08,\n",
       "        -3.97404612e+08, -8.77658746e+08, -5.05213180e+08, -3.95027283e+08,\n",
       "        -0.00000000e+00, -3.12084185e+03, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00]),\n",
       " 'split1_train_score': array([-1.08820969e+09, -5.70378318e+08, -4.17983068e+08, -9.46030255e+08,\n",
       "        -5.21615374e+08, -3.90429982e+08, -9.16152771e+08, -4.91674868e+08,\n",
       "        -3.64857771e+08, -9.07498327e+08, -4.99554359e+08, -3.94274167e+08,\n",
       "        -0.00000000e+00, -7.41918389e-01, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00]),\n",
       " 'split2_train_score': array([-1.15760232e+09, -5.72422550e+08, -4.29430805e+08, -9.25051799e+08,\n",
       "        -5.37422628e+08, -3.96522718e+08, -9.43436648e+08, -4.86186989e+08,\n",
       "        -3.73669826e+08, -9.18306664e+08, -5.16141082e+08, -3.77013328e+08,\n",
       "        -2.65690975e+03, -2.25586677e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00]),\n",
       " 'split3_train_score': array([-1.17207577e+09, -5.86031496e+08, -4.27372966e+08, -9.46756160e+08,\n",
       "        -5.01359976e+08, -3.96978665e+08, -9.74380508e+08, -5.04344222e+08,\n",
       "        -3.87062081e+08, -9.07451739e+08, -4.87361232e+08, -3.85826825e+08,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00]),\n",
       " 'split4_train_score': array([-1.05297231e+09, -5.75464288e+08, -4.22511702e+08, -8.83869649e+08,\n",
       "        -5.18215356e+08, -3.92938785e+08, -8.70185162e+08, -4.91571631e+08,\n",
       "        -3.79922558e+08, -8.59899334e+08, -4.91628504e+08, -3.82059203e+08,\n",
       "        -2.03549500e+01, -2.36941711e+02, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00]),\n",
       " 'mean_train_score': array([-1.11371474e+09, -5.74482418e+08, -4.28096948e+08, -9.25890816e+08,\n",
       "        -5.25831772e+08, -3.94491626e+08, -9.37383452e+08, -4.92543212e+08,\n",
       "        -3.80583370e+08, -8.94162962e+08, -4.99979671e+08, -3.86840161e+08,\n",
       "        -5.35452940e+02, -6.72156269e+02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00]),\n",
       " 'std_train_score': array([4.45607823e+07, 6.26051731e+06, 8.52612953e+06, 2.28511903e+07,\n",
       "        1.68509589e+07, 2.46661531e+06, 4.10633742e+07, 6.23471641e+06,\n",
       "        1.11396240e+07, 2.18345015e+07, 1.01757429e+07, 6.96770361e+06,\n",
       "        1.06075770e+03, 1.22774840e+03, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00])}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "cvres  \n",
    "# is a dictionary with certain key value pairs"
   ]
  },
  {
   "source": [
    "### The grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but when the hyperparameter search space is large, it is often preferable to use RandomizedSearchCV instead.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "### It works simmillary with GridSearch class , but has to main advantages:\n",
    "    - If you let the randomized search run for, say, 1,000 iterations, this approach will explore 1,000 different values for each hyperparameter (instead of just a few values per hyperparameter with the grid search approach).\n",
    "    - You have more control over the computing budget you want to allocate to hyperparameter search, simply by setting the number of iterations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distribs = {\n",
    "        'n_estimators': randint(low=1, high=200),\n",
    "        'max_features': randint(low=1, high=8),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42),\n",
       "                   param_distributions={'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001E0043C7DF0>,\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001E0044B62E0>},\n",
       "                   random_state=42, scoring='neg_mean_squared_error')"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n",
    "                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "rnd_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "49150.70756927707 {'max_features': 7, 'n_estimators': 180}\n51389.889203389284 {'max_features': 5, 'n_estimators': 15}\n50796.155224308866 {'max_features': 3, 'n_estimators': 72}\n50835.13360315349 {'max_features': 5, 'n_estimators': 21}\n49280.9449827171 {'max_features': 7, 'n_estimators': 122}\n50774.90662363929 {'max_features': 3, 'n_estimators': 75}\n50682.78888164288 {'max_features': 3, 'n_estimators': 88}\n49608.99608105296 {'max_features': 5, 'n_estimators': 100}\n50473.61930350219 {'max_features': 3, 'n_estimators': 150}\n64429.84143294435 {'max_features': 5, 'n_estimators': 2}\n"
     ]
    }
   ],
   "source": [
    "cvres = rnd_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}